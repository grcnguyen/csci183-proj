{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 183 Data Science\n",
    "### Spam Filtering for Short Messages: Features Matrix\n",
    "#### Ryan Johnson, Grace Nguyen, and Raya Young\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "#import sets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the test data \n",
    "Separate into two arrays: spam and ham. These arrays will be processed individually in order to generate word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data/spamcollectiondata.tsv\", sep='\\t', names = [\"Category\", \"Message\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_data = [word.lower() for word in data['Message']]\n",
    "category = data['Category'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal and Stemming\n",
    "Clean both sets of data by removing stopwords. This way, the word cloud will not be completely populated by common stop words. Stemming is also important to ensure eliminate the possibility of having multiple different forms of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['go', 'jurong', 'point,', 'crazy..', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amor', 'wat...'], 'ham'], [['ok', 'lar...', 'joke', 'wif', 'u', 'oni...'], 'ham'], [['free', 'entri', '2', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', '87121', 'receiv', 'entri', 'question(std', 'txt', 'rate)t&c', 'appli', '08452810075over18'], 'spam'], [['u', 'dun', 'say', 'earli', 'hor...', 'u', 'c', 'alreadi', 'say...'], 'ham'], [['nah', \"don't\", 'think', 'goe', 'usf,', 'live', 'around', 'though'], 'ham'], [['freemsg', 'hey', 'darl', 'it', '3', 'week', 'word', 'back!', \"i'd\", 'like', 'fun', 'still?', 'tb', 'ok!', 'xxx', 'std', 'chgs', 'send,', '£1.50', 'rcv'], 'spam'], [['even', 'brother', 'like', 'speak', 'me.', 'treat', 'like', 'aid', 'patent.'], 'ham'], [['per', 'request', 'mell', 'mell', '(oru', 'minnaminungint', 'nurungu', 'vettam)', 'set', 'callertun', 'callers.', 'press', '*9', 'copi', 'friend', 'callertun'], 'ham'], [['winner!!', 'valu', 'network', 'custom', 'select', 'receivea', '£900', 'prize', 'reward!', 'claim', 'call', '09061701461.', 'claim', 'code', 'kl341.', 'valid', '12', 'hour', 'only.'], 'spam'], [['mobil', '11', 'month', 'more?', 'u', 'r', 'entitl', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free!', 'call', 'mobil', 'updat', 'co', 'free', '08002986030'], 'spam']]\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "training_set = []\n",
    "i = 0\n",
    "for message in message_data:\n",
    "    sentence = message.split(\" \")\n",
    "    filtered = []\n",
    "    pr = []\n",
    "    for word in sentence:\n",
    "        if word.lower() not in stop:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            filtered.append(stemmed)\n",
    "    pr.append(filtered)\n",
    "    pr.append(category[i])\n",
    "    training_set.append(pr)\n",
    "    i = i+1\n",
    "    \n",
    "#print first 10 elements in training_set to see format\n",
    "print(training_set[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convert training_set to Dictionary\n",
    "\n",
    "NLTK's classifiers only accept texts in hashable formats, so we need to convert our list into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(words_list):\n",
    "  return dict([(word, True) for word in words_list])\n",
    " \n",
    "training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in training_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert dictionary to a X matrix and Y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_vector(training_set):\n",
    "    words_vector = [] \n",
    "    for review in training_set:\n",
    "        for word in review[0]:\n",
    "            if word not in words_vector: words_vector.append(word) \n",
    "    return words_vector\n",
    " \n",
    "def generate_Y_vector(training_set, training_class):\n",
    "    no_reviews = len(training_set)\n",
    "    Y = np.zeros(shape=no_reviews)\n",
    "    for ii in range(0,no_reviews):\n",
    "        review_class = training_set[ii][1]\n",
    "        Y[ii] = 1 if review_class == training_class else 0\n",
    "    return Y\n",
    "        \n",
    "def generate_X_matrix(training_set, words_vector):\n",
    "    no_reviews = len(training_set)\n",
    "    no_words = len(words_vector)\n",
    "    X = np.zeros(shape=(no_reviews, no_words+1))\n",
    "    for ii in range(0,no_reviews):\n",
    "        X[ii][0] = 1\n",
    "        review_text = training_set[ii][0]\n",
    "        total_words_in_review = len(review_text)\n",
    "        rt = list(review_text)\n",
    "        for word in rt:\n",
    "            word_occurences = rt.count(word)\n",
    "            word_index = words_vector.index(word)+1\n",
    "            X[ii][word_index] = word_occurences / float(total_words_in_review)\n",
    "    return X\n",
    "\n",
    "words_vector = generate_words_vector(training_set_formatted)\n",
    "X = generate_X_matrix(training_set_formatted, words_vector)\n",
    "Y_neg = generate_Y_vector(training_set_formatted, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
