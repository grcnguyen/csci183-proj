{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 183 Data Science\n",
    "### Spam Filtering for Short Messages: Features Matrix\n",
    "#### Ryan Johnson, Grace Nguyen, and Raya Young\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the test data \n",
    "Separate into two arrays: spam and ham. These arrays will be processed individually in order to generate word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data/spamcollectiondata.tsv\", sep='\\t', names = [\"Category\", \"Message\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_data = [word.lower() for word in data['Message']]\n",
    "category = data['Category'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal and Stemming\n",
    "Clean both sets of data by removing stopwords. This way, the word cloud will not be completely populated by common stop words. Stemming is also important to ensure eliminate the possibility of having multiple different forms of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['go', 'jurong', 'point,', 'crazy..', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amor', 'wat...'], 'ham'], [['ok', 'lar...', 'joke', 'wif', 'u', 'oni...'], 'ham'], [['free', 'entri', '2', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', '87121', 'receiv', 'entri', 'question(std', 'txt', 'rate)t&c', 'appli', '08452810075over18'], 'spam'], [['u', 'dun', 'say', 'earli', 'hor...', 'u', 'c', 'alreadi', 'say...'], 'ham'], [['nah', \"don't\", 'think', 'goe', 'usf,', 'live', 'around', 'though'], 'ham'], [['freemsg', 'hey', 'darl', 'it', '3', 'week', 'word', 'back!', \"i'd\", 'like', 'fun', 'still?', 'tb', 'ok!', 'xxx', 'std', 'chgs', 'send,', '£1.50', 'rcv'], 'spam'], [['even', 'brother', 'like', 'speak', 'me.', 'treat', 'like', 'aid', 'patent.'], 'ham'], [['per', 'request', 'mell', 'mell', '(oru', 'minnaminungint', 'nurungu', 'vettam)', 'set', 'callertun', 'callers.', 'press', '*9', 'copi', 'friend', 'callertun'], 'ham'], [['winner!!', 'valu', 'network', 'custom', 'select', 'receivea', '£900', 'prize', 'reward!', 'claim', 'call', '09061701461.', 'claim', 'code', 'kl341.', 'valid', '12', 'hour', 'only.'], 'spam'], [['mobil', '11', 'month', 'more?', 'u', 'r', 'entitl', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free!', 'call', 'mobil', 'updat', 'co', 'free', '08002986030'], 'spam']]\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "training_set = []\n",
    "i = 0\n",
    "for message in message_data:\n",
    "    sentence = message.split(\" \")\n",
    "    filtered = []\n",
    "    pr = []\n",
    "    for word in sentence:\n",
    "        if word.lower() not in stop:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            filtered.append(stemmed)\n",
    "    pr.append(filtered)\n",
    "    pr.append(category[i])\n",
    "    training_set.append(pr)\n",
    "    i = i+1\n",
    "    \n",
    "#print first 10 elements in training_set to see format\n",
    "print(training_set[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert y to 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(training_set)\n",
    "train_df.columns = ['Lists','Labels']\n",
    "train_df.head()\n",
    "y = np.array(train_df.Labels)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'ham':\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "        \n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 5572 12236]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-12208257fd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 5572 12236]"
     ]
    }
   ],
   "source": [
    "X_train, X_tets, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train,y_train)\n",
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convert training_set to Dictionary\n",
    "\n",
    "NLTK's classifiers only accept texts in hashable formats, so we need to convert our list into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(words_list):\n",
    "  return dict([(word, True) for word in words_list])\n",
    " \n",
    "training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in training_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert dictionary to a X matrix and Y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_words_vector(training_set):\n",
    "    words_vector = [] \n",
    "    for review in training_set:\n",
    "        for word in review[0]:\n",
    "            if word not in words_vector: words_vector.append(word) \n",
    "    return words_vector\n",
    " \n",
    "def generate_Y_vector(training_set, training_class):\n",
    "    no_reviews = len(training_set)\n",
    "    Y = np.zeros(shape=no_reviews)\n",
    "    for ii in range(0,no_reviews):\n",
    "        review_class = training_set[ii][1]\n",
    "        if review_class == training_class:\n",
    "            Y[ii] = 1 \n",
    "        else:\n",
    "            Y[ii] = 0\n",
    "    return Y\n",
    "        \n",
    "def generate_X_matrix(training_set, words_vector):\n",
    "    no_reviews = len(training_set)\n",
    "    no_words = len(words_vector)\n",
    "    X = np.zeros(shape=(no_reviews, no_words+1))\n",
    "    for ii in range(0,no_reviews):\n",
    "        X[ii][0] = 1\n",
    "        review_text = training_set[ii][0]\n",
    "        total_words_in_review = len(review_text)\n",
    "        rt = list(review_text)\n",
    "        for word in rt:\n",
    "            word_occurences = rt.count(word)\n",
    "            word_index = words_vector.index(word)+1\n",
    "            X[ii][word_index] = word_occurences / float(total_words_in_review)\n",
    "    return X\n",
    "\n",
    "words_vector = generate_words_vector(training_set_formatted)\n",
    "X = np.array(generate_X_matrix(training_set_formatted, words_vector))\n",
    "Y_neg = np.array(generate_Y_vector(training_set_formatted, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 5572 12236]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-49a33c4f3b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/_Turing/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 5572 12236]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perform Logistic Regression on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "# @param a                 double\n",
    "\n",
    "def sigmoidFunction(a):\n",
    "    \n",
    "    #return 1.0 / (1.0 + np.exp(-a))\n",
    "    return 1.0 / (1.0 + 2.718281828549**(-a))\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Cost Function\n",
    "# @param X                 nxm Matrix\n",
    "# @param y                 1xm Vector\n",
    "# @param theta             1xn Vector\n",
    "# @return\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    z = np.dot(np.transpose(X),theta)               \n",
    "           \n",
    "    cost = (-1/m)*((y.T.dot(np.log(sigmoidFunction(z)))) + ((1 - y.T).dot(np.log(1 - sigmoidFunction(z)))))           \n",
    "    return cost\n",
    "    \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # theta * X    (1xn * nxm = 1xm)\n",
    "    hypothesis = (np.array(theta) * np.array(X)).tolist()\n",
    "\n",
    "    # h = sigmoid(theta * X)\n",
    "    for idx in range(m):\n",
    "        hypothesis[idx] = sigmoidFunction(hypothesis[idx])\n",
    "\n",
    "    # log(h) * y' + log(1 - h) * (1 - y)'\n",
    "    sum = 0\n",
    "    for idx in range(m):\n",
    "        sum += y[idx] * math.log(hypothesis[idx]) + (1 - y[idx]) * math.log(1 - hypothesis[idx])\n",
    "\n",
    "    # -(log(h) * y' + log(1 - h) * (1 - y)') / m\n",
    "    return -sum / m\n",
    "    \"\"\"\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Gradient Descent\n",
    "# @param X                 nxm Matrix\n",
    "# @param y                 1xm Vector\n",
    "# @param theta             1xn Vector\n",
    "# @param alpha             step factor\n",
    "# @param maxInteration     Maximum iteration for convergence\n",
    "# @return\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, maxIteration):\n",
    "    \n",
    "    # m= number of training data, n= number of features\n",
    "    m = len(y)\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    \n",
    "    costHistory = []\n",
    "\n",
    "    for it in range(maxIteration):\n",
    "\n",
    "        # size: 1 x n\n",
    "        temp_theta = np.zeros(n)\n",
    "        \n",
    "        # size: 1 x m\n",
    "        hypothesis = np.zeros(m)\n",
    "\n",
    "        # get values for hypothesis vector\n",
    "        for i in range(m):\n",
    "            hypothesis[i] = sigmoidFunction(np.dot(theta,np.transpose(X)[i]))\n",
    "\n",
    "        # for each feature, do gradient descent\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "\n",
    "            #sum = 0\n",
    "            #tempsum = np.dot(np.array(hypothesis)-np.array(y),np.array(X[i]))\n",
    "            #tempsum = np.dot(hypothesis-y,X)\n",
    "            \n",
    "            #tempsum = (hypothesis-y)*X\n",
    "            \n",
    "            #for j in range(m-1):\n",
    "             #   sum += (hypothesis[j] - y[j])*X[i][j]\n",
    "\n",
    "            temp_theta[i] = theta[i] - (alpha/m)*tempsum\n",
    "            print(tempsum)\n",
    "        \"\"\"\n",
    "        temp_theta = theta - (alpha/m)*np.dot(X,hypothesis-y)\n",
    "    \n",
    "        # update theta\n",
    "        theta = temp_theta\n",
    "\n",
    "        # compute and record cost\n",
    "        costHistory[it] = computeCost(X, y, theta)\n",
    "        print(\"Done iteration: {}.\".format(it))\n",
    "\n",
    "    return costHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if len(np.transpose(X)) != len(y):\n",
    "#    X = np.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 12236 x 5572\n",
      "y: 5572 x 1\n"
     ]
    }
   ],
   "source": [
    "print(\"X: {} x {}\".format(len(X),len(X[0])))\n",
    "print(\"y: {} x {}\".format(len(y),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put our new X matrix and Y vector into the gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-cf2b03eb6780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcostHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-5a3110bbf547>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(X, y, theta, alpha, maxIteration)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# compute and record cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mcostHistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done iteration: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-5a3110bbf547>\u001b[0m in \u001b[0;36mcomputeCost\u001b[0;34m(X, y, theta)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoidFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoidFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'log'"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(len(X))\n",
    "\n",
    "\n",
    "costHistory = gradientDescent(X,y,theta,.01,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.dot(theta,np.transpose(X)[200]))\n",
    "len(y)\n",
    "#print(np.dot(y,np.transpose(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unexpected error: (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x108f9dd88>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplot\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from sklearn import linear_model\n",
    "\n",
    "class IrisDatasetLogisticRegression:\n",
    "        \"\"\"\n",
    "         Implementing logistic regression using Iris dataset\n",
    "        \"\"\"     \n",
    "        \n",
    "        \"\"\"Global class variables\"\"\"\n",
    "        \"Matrix containing set of features\"\n",
    "        X = None\n",
    "        \n",
    "        \"Matrix containing set of outputs\"\n",
    "        y = None\n",
    "        \n",
    "        def __init__(self, X, y):\n",
    "            \"\"\" USAGE:\n",
    "            Default constructor            \n",
    "           \n",
    "            PARAMETERS:\n",
    "            X - feature matrix\n",
    "            y - output matrix     \n",
    "            \n",
    "            RETURN:            \n",
    "            \"\"\"          \n",
    "            self.X = X\n",
    "            self.y = y            \n",
    "            \n",
    "            \"\"\"Convert y into a proper 2 dimensional array/matrix. This is to facilitate proper matrix arithmetics.\"\"\"\n",
    "            if len(y.shape) == 1:\n",
    "                y.shape = (y.shape[0],1)\n",
    "            \n",
    "                    \n",
    "        def sigmoid(self,z):\n",
    "            \"\"\" USAGE:\n",
    "            Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).            \n",
    "           \n",
    "            PARAMETERS:\n",
    "            z - Matrix, vector or scalar       \n",
    "            \n",
    "            RETURN:\n",
    "            The sigmoid value\n",
    "            \"\"\"    \n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        \n",
    "        \n",
    "        def compute_cost(self,X, y, theta):\n",
    "            \"\"\" USAGE:\n",
    "            Define the cost function           \n",
    "          \n",
    "            PARAMETERS:\n",
    "            X - Features\n",
    "            y - Output\n",
    "            theta        \n",
    "           \n",
    "            RETURN:\n",
    "            return the cost function value\n",
    "            \"\"\"    \n",
    "            m = X.shape[0]\n",
    "            z = np.dot(X,theta)            \n",
    "            h = self.sigmoid(z);    \n",
    "           \n",
    "            J=(float(-1)/m)*((y.T.dot(np.log(h))) + ((1 - y.T).dot(np.log(1 - h))))           \n",
    "            return J\n",
    "        \n",
    "        \n",
    "        def compute_gradient(self,X, y, theta):\n",
    "            \"\"\" USAGE:                  \n",
    "            Compute the gradient using vectorization.\n",
    "            \n",
    "            PARAMETERS:           \n",
    "            X - Features\n",
    "            y - Output\n",
    "            theta \n",
    "            \n",
    "            RETURN:           \n",
    "            \"\"\"    \n",
    "            m = X.shape[0]\n",
    "            z = np.dot(X,theta)            \n",
    "            h = self.sigmoid(z);\n",
    "    \n",
    "            grad = (float(1)/m)*((h-y).T.dot(X))          \n",
    "            return grad\n",
    "        \n",
    "\n",
    "        def plot_two_features(self):\n",
    "            \"\"\" USAGE:\n",
    "            Plot first two features from the Iris dataset  \n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"      \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, title =\"Iris Dataset - Plotting two features\", xlabel = 'Sepal Length', ylabel='Sepal Width')\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "                     \n",
    "                        \n",
    "            setosa = np.where(self.y == 0)\n",
    "            versicolour = np.where(self.y ==1)\n",
    "            \n",
    "            ax.scatter(X[setosa, 0], X[setosa, 1], s=20, c='r', marker ='o')  \n",
    "            ax.scatter(X[versicolour, 0], X[versicolour, 1], s=20, c='r', marker ='x') \n",
    "            plt.legend( ('Iris Type - Setosa', 'Iris Type - Versicolour') )\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "        def plot_three_features(self):\n",
    "            \"\"\" USAGE:\n",
    "            Plot first two features from the Iris dataset  \n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"      \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, title =\"Iris Dataset - Plotting three features\", xlabel = 'Sepal Length', ylabel='Sepal Width', zlabel='Petal Length', projection='3d')\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            plt.setp(ax.get_zticklabels(), visible=False)\n",
    "           \n",
    "            ax.scatter(X[:, 0], X[:, 1],X[:, 2], s=20, c='r', marker ='o')                   \n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        def run_logistic_regression(self):\n",
    "            \"\"\" USAGE:\n",
    "            Apply principles of logistic regression\n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"  \n",
    "            \n",
    "            \"\"\"m= number of training data, n= number of features\"\"\"\n",
    "            m = X.shape[0]\n",
    "            n = X.shape[1]\n",
    "            \n",
    "            \"\"\"Add intercept term to X\"\"\"\n",
    "            self.X = np.hstack((np.ones((m, 1)), self.X))\n",
    "            \n",
    "            \n",
    "            \"\"\"Initialize fitting parameters. Take into account the intercept term.\"\"\"\n",
    "            initial_theta = np.zeros((n + 1, 1))\n",
    "            \n",
    "            \"\"\"\"Compute initial cost and gradient\"\"\"\n",
    "            cost = self.compute_cost(self.X, self.y, initial_theta)            \n",
    "            gradient = self.compute_gradient(self.X, self.y, initial_theta)\n",
    "            \n",
    "            print ('Cost at initial theta (zeros): {0} \\nGradient at initial theta (zeros): {1}'.format(cost, gradient))\n",
    "        \n",
    "            def f(theta):\n",
    "                return np.ndarray.flatten(self.compute_cost(self.X, self.y, initial_theta))\n",
    "            \n",
    "            def fprime(theta):\n",
    "                return np.ndarray.flatten(self.compute_gradient(self.X, self.y, initial_theta))\n",
    "            \n",
    "            print(fmin_bfgs(f, initial_theta, fprime, disp=True, maxiter=400, full_output = True, retall=True))        \n",
    "          \n",
    "           \n",
    "try:\n",
    "#    iris = datasets.load_iris()\n",
    "#    X = iris.data\n",
    "#    Y = iris.target\n",
    "    \n",
    "    data = np.loadtxt('data/ex2data1.txt', delimiter=',')\n",
    "    X = data[:, 0:2]   \n",
    "    Y = data[:, 2]\n",
    "   \n",
    "    \n",
    "    logistic_regression  = IrisDatasetLogisticRegression(X, Y)\n",
    "    #logistic_regression.plot_two_features()\n",
    "    #logistic_regression.plot_three_features()\n",
    "    \n",
    "    logistic_regression.run_logistic_regression()\n",
    "    \n",
    "except:\n",
    "    print (\"unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
