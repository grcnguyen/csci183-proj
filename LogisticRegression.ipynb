{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 183 Data Science\n",
    "### Spam Filtering for Short Messages: Features Matrix\n",
    "#### Ryan Johnson, Grace Nguyen, and Raya Young\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the test data \n",
    "Separate into two arrays: spam and ham. These arrays will be processed individually in order to generate word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data/spamcollectiondata.tsv\", sep='\\t', names = [\"Category\", \"Message\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_data = [word.lower() for word in data['Message']]\n",
    "category = data['Category'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal and Stemming\n",
    "Clean both sets of data by removing stopwords. This way, the word cloud will not be completely populated by common stop words. Stemming is also important to ensure eliminate the possibility of having multiple different forms of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['go', 'jurong', 'point,', 'crazy..', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amor', 'wat...'], 'ham'], [['ok', 'lar...', 'joke', 'wif', 'u', 'oni...'], 'ham'], [['free', 'entri', '2', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', '87121', 'receiv', 'entri', 'question(std', 'txt', 'rate)t&c', 'appli', '08452810075over18'], 'spam'], [['u', 'dun', 'say', 'earli', 'hor...', 'u', 'c', 'alreadi', 'say...'], 'ham'], [['nah', \"don't\", 'think', 'goe', 'usf,', 'live', 'around', 'though'], 'ham'], [['freemsg', 'hey', 'darl', 'it', '3', 'week', 'word', 'back!', \"i'd\", 'like', 'fun', 'still?', 'tb', 'ok!', 'xxx', 'std', 'chgs', 'send,', '£1.50', 'rcv'], 'spam'], [['even', 'brother', 'like', 'speak', 'me.', 'treat', 'like', 'aid', 'patent.'], 'ham'], [['per', 'request', 'mell', 'mell', '(oru', 'minnaminungint', 'nurungu', 'vettam)', 'set', 'callertun', 'callers.', 'press', '*9', 'copi', 'friend', 'callertun'], 'ham'], [['winner!!', 'valu', 'network', 'custom', 'select', 'receivea', '£900', 'prize', 'reward!', 'claim', 'call', '09061701461.', 'claim', 'code', 'kl341.', 'valid', '12', 'hour', 'only.'], 'spam'], [['mobil', '11', 'month', 'more?', 'u', 'r', 'entitl', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free!', 'call', 'mobil', 'updat', 'co', 'free', '08002986030'], 'spam']]\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "training_set = []\n",
    "i = 0\n",
    "for message in message_data:\n",
    "    sentence = message.split(\" \")\n",
    "    filtered = []\n",
    "    pr = []\n",
    "    for word in sentence:\n",
    "        if word.lower() not in stop:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            filtered.append(stemmed)\n",
    "    pr.append(filtered)\n",
    "    pr.append(category[i])\n",
    "    training_set.append(pr)\n",
    "    i = i+1\n",
    "    \n",
    "#print first 10 elements in training_set to see format\n",
    "print(training_set[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Convert training_set to Dictionary\n",
    "\n",
    "NLTK's classifiers only accept texts in hashable formats, so we need to convert our list into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(words_list):\n",
    "  return dict([(word, True) for word in words_list])\n",
    " \n",
    "training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in training_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert dictionary to a X matrix and Y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_words_vector(training_set):\n",
    "    words_vector = [] \n",
    "    for review in training_set:\n",
    "        for word in review[0]:\n",
    "            if word not in words_vector: words_vector.append(word) \n",
    "    return words_vector\n",
    " \n",
    "def generate_Y_vector(training_set, training_class):\n",
    "    no_reviews = len(training_set)\n",
    "    Y = np.zeros(shape=no_reviews)\n",
    "    for ii in range(0,no_reviews):\n",
    "        review_class = training_set[ii][1]\n",
    "        Y[ii] = 1 if review_class == training_class else 0\n",
    "    return Y\n",
    "        \n",
    "def generate_X_matrix(training_set, words_vector):\n",
    "    no_reviews = len(training_set)\n",
    "    no_words = len(words_vector)\n",
    "    X = np.zeros(shape=(no_reviews, no_words+1))\n",
    "    for ii in range(0,no_reviews):\n",
    "        X[ii][0] = 1\n",
    "        review_text = training_set[ii][0]\n",
    "        total_words_in_review = len(review_text)\n",
    "        rt = list(review_text)\n",
    "        for word in rt:\n",
    "            word_occurences = rt.count(word)\n",
    "            word_index = words_vector.index(word)+1\n",
    "            X[ii][word_index] = word_occurences / float(total_words_in_review)\n",
    "    return X\n",
    "\n",
    "words_vector = generate_words_vector(training_set_formatted)\n",
    "X = generate_X_matrix(training_set_formatted, words_vector)\n",
    "Y_neg = generate_Y_vector(training_set_formatted, 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perform Logistic Regression on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "# @param x                 double\n",
    "\n",
    "def sigmoidFunction(a):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Cost Function\n",
    "# @param X                 nxm Matrix\n",
    "# @param y                 1xm Vector\n",
    "# @param theta             1xn Vector\n",
    "# @return\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    # theta * X    (1xn * nxm = 1xm)\n",
    "    hypothesis = (np.array(theta) * np.array(X)).tolist()\n",
    "\n",
    "    # h = sigmoid(theta * X)\n",
    "    for idx in range(m):\n",
    "        hypothesis[idx] = sigmoidFunction(hypothesis[idx])\n",
    "\n",
    "    # log(h) * y' + log(1 - h) * (1 - y)'\n",
    "    sum = 0\n",
    "    for idx in range(m):\n",
    "        sum += y[idx] * math.log(hypothesis[idx]) + (1 - y[idx]) * math.log(1 - hypothesis[idx])\n",
    "\n",
    "    # -(log(h) * y' + log(1 - h) * (1 - y)') / m\n",
    "    return -sum / m\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Gradient Descent\n",
    "# @param X                 nxm Matrix\n",
    "# @param y                 1xm Vector\n",
    "# @param theta             1xn Vector\n",
    "# @param alpha             step factor\n",
    "# @param maxInteration     Maximum iteration for convergence\n",
    "# @return\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, maxIteration):\n",
    "    \n",
    "    n = len(X)\n",
    "    m = len(y)\n",
    "    \n",
    "    # size: maxIteration\n",
    "    costHistory = []\n",
    "\n",
    "    for it in range(maxIteration):\n",
    "\n",
    "        # size: n\n",
    "        temp_theta = np.zeros(n)\n",
    "\n",
    "        # size: m\n",
    "        hypothesis = np.zeros(m)\n",
    "\n",
    "        # get values for hypothesis vector\n",
    "        for i in range(m-1):\n",
    "            \n",
    "            # g_value = theta^T * X[i] <-- dot product\n",
    "            g_value = np.dot(np.array(theta), np.array(X[i]))\n",
    "\n",
    "            #g_value = 0\n",
    "            #for j in range(n-1):\n",
    "            #    g_value += theta[j]*X[j][i]\n",
    "            \n",
    "            hypothesis[i] = sigmoidFunction(g_value)\n",
    "\n",
    "        # for each feature, do gradient descent\n",
    "        for i in range(n):\n",
    "\n",
    "            #sum = 0\n",
    "            tempsum = np.dot(np.array(hypothesis)-np.array(y),np.array(X[i]))\n",
    "            #for j in range(m-1):\n",
    "             #   sum += (hypothesis[j] - y[j])*X[i][j]\n",
    "\n",
    "            temp_theta[i] = theta[i] - (alpha/m)*tempsum\n",
    "            print(tempsum)\n",
    "\n",
    "        # update theta\n",
    "        for i in range(n-1):\n",
    "            theta[i] = temp_theta[i]\n",
    "\n",
    "        # compute and record cost\n",
    "        costHistory[it] = computeCost(X, y, theta)\n",
    "        print(\"Done iteration: {}. Current Cost: {}.\".format(it,costHistory[it]))\n",
    "\n",
    "    return costHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put our new X matrix and Y vector into the gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5572,) and (12236,) not aligned: 5572 (dim 0) != 12236 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e1e74e6af811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3e4b461c551f>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(X, y, theta, alpha, maxIteration)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m#sum = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtempsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;31m#for j in range(m-1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m              \u001b[0;31m#   sum += (hypothesis[j] - y[j])*X[i][j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5572,) and (12236,) not aligned: 5572 (dim 0) != 12236 (dim 0)"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(len(X))\n",
    "X = np.matrix(X)\n",
    "X = np.transpose(X)\n",
    "X = X.tolist()\n",
    "gradientDescent(X,Y_neg,theta,.01,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unexpected error: (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x108f9dd88>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplot\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from sklearn import linear_model\n",
    "\n",
    "class IrisDatasetLogisticRegression:\n",
    "        \"\"\"\n",
    "         Implementing logistic regression using Iris dataset\n",
    "        \"\"\"     \n",
    "        \n",
    "        \"\"\"Global class variables\"\"\"\n",
    "        \"Matrix containing set of features\"\n",
    "        X = None\n",
    "        \n",
    "        \"Matrix containing set of outputs\"\n",
    "        y = None\n",
    "        \n",
    "        def __init__(self, X, y):\n",
    "            \"\"\" USAGE:\n",
    "            Default constructor            \n",
    "           \n",
    "            PARAMETERS:\n",
    "            X - feature matrix\n",
    "            y - output matrix     \n",
    "            \n",
    "            RETURN:            \n",
    "            \"\"\"          \n",
    "            self.X = X\n",
    "            self.y = y            \n",
    "            \n",
    "            \"\"\"Convert y into a proper 2 dimensional array/matrix. This is to facilitate proper matrix arithmetics.\"\"\"\n",
    "            if len(y.shape) == 1:\n",
    "                y.shape = (y.shape[0],1)\n",
    "            \n",
    "                    \n",
    "        def sigmoid(self,z):\n",
    "            \"\"\" USAGE:\n",
    "            Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).            \n",
    "           \n",
    "            PARAMETERS:\n",
    "            z - Matrix, vector or scalar       \n",
    "            \n",
    "            RETURN:\n",
    "            The sigmoid value\n",
    "            \"\"\"    \n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        \n",
    "        \n",
    "        def compute_cost(self,X, y, theta):\n",
    "            \"\"\" USAGE:\n",
    "            Define the cost function           \n",
    "          \n",
    "            PARAMETERS:\n",
    "            X - Features\n",
    "            y - Output\n",
    "            theta        \n",
    "           \n",
    "            RETURN:\n",
    "            return the cost function value\n",
    "            \"\"\"    \n",
    "            m = X.shape[0]\n",
    "            z = np.dot(X,theta)            \n",
    "            h = self.sigmoid(z);    \n",
    "           \n",
    "            J=(float(-1)/m)*((y.T.dot(np.log(h))) + ((1 - y.T).dot(np.log(1 - h))))           \n",
    "            return J\n",
    "        \n",
    "        \n",
    "        def compute_gradient(self,X, y, theta):\n",
    "            \"\"\" USAGE:                  \n",
    "            Compute the gradient using vectorization.\n",
    "            \n",
    "            PARAMETERS:           \n",
    "            X - Features\n",
    "            y - Output\n",
    "            theta \n",
    "            \n",
    "            RETURN:           \n",
    "            \"\"\"    \n",
    "            m = X.shape[0]\n",
    "            z = np.dot(X,theta)            \n",
    "            h = self.sigmoid(z);\n",
    "    \n",
    "            grad = (float(1)/m)*((h-y).T.dot(X))          \n",
    "            return grad\n",
    "        \n",
    "\n",
    "        def plot_two_features(self):\n",
    "            \"\"\" USAGE:\n",
    "            Plot first two features from the Iris dataset  \n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"      \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, title =\"Iris Dataset - Plotting two features\", xlabel = 'Sepal Length', ylabel='Sepal Width')\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "                     \n",
    "                        \n",
    "            setosa = np.where(self.y == 0)\n",
    "            versicolour = np.where(self.y ==1)\n",
    "            \n",
    "            ax.scatter(X[setosa, 0], X[setosa, 1], s=20, c='r', marker ='o')  \n",
    "            ax.scatter(X[versicolour, 0], X[versicolour, 1], s=20, c='r', marker ='x') \n",
    "            plt.legend( ('Iris Type - Setosa', 'Iris Type - Versicolour') )\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "        def plot_three_features(self):\n",
    "            \"\"\" USAGE:\n",
    "            Plot first two features from the Iris dataset  \n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"      \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, title =\"Iris Dataset - Plotting three features\", xlabel = 'Sepal Length', ylabel='Sepal Width', zlabel='Petal Length', projection='3d')\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            plt.setp(ax.get_zticklabels(), visible=False)\n",
    "           \n",
    "            ax.scatter(X[:, 0], X[:, 1],X[:, 2], s=20, c='r', marker ='o')                   \n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        def run_logistic_regression(self):\n",
    "            \"\"\" USAGE:\n",
    "            Apply principles of logistic regression\n",
    "          \n",
    "            PARAMETERS:           \n",
    "            \n",
    "            RETURN:    \n",
    "            \"\"\"  \n",
    "            \n",
    "            \"\"\"m= number of training data, n= number of features\"\"\"\n",
    "            m = X.shape[0]\n",
    "            n = X.shape[1]\n",
    "            \n",
    "            \"\"\"Add intercept term to X\"\"\"\n",
    "            self.X = np.hstack((np.ones((m, 1)), self.X))\n",
    "            \n",
    "            \n",
    "            \"\"\"Initialize fitting parameters. Take into account the intercept term.\"\"\"\n",
    "            initial_theta = np.zeros((n + 1, 1))\n",
    "            \n",
    "            \"\"\"\"Compute initial cost and gradient\"\"\"\n",
    "            cost = self.compute_cost(self.X, self.y, initial_theta)            \n",
    "            gradient = self.compute_gradient(self.X, self.y, initial_theta)\n",
    "            \n",
    "            print ('Cost at initial theta (zeros): {0} \\nGradient at initial theta (zeros): {1}'.format(cost, gradient))\n",
    "        \n",
    "            def f(theta):\n",
    "                return np.ndarray.flatten(self.compute_cost(self.X, self.y, initial_theta))\n",
    "            \n",
    "            def fprime(theta):\n",
    "                return np.ndarray.flatten(self.compute_gradient(self.X, self.y, initial_theta))\n",
    "            \n",
    "            print(fmin_bfgs(f, initial_theta, fprime, disp=True, maxiter=400, full_output = True, retall=True))        \n",
    "          \n",
    "           \n",
    "try:\n",
    "#    iris = datasets.load_iris()\n",
    "#    X = iris.data\n",
    "#    Y = iris.target\n",
    "    \n",
    "    data = np.loadtxt('data/ex2data1.txt', delimiter=',')\n",
    "    X = data[:, 0:2]   \n",
    "    Y = data[:, 2]\n",
    "   \n",
    "    \n",
    "    logistic_regression  = IrisDatasetLogisticRegression(X, Y)\n",
    "    #logistic_regression.plot_two_features()\n",
    "    #logistic_regression.plot_three_features()\n",
    "    \n",
    "    logistic_regression.run_logistic_regression()\n",
    "    \n",
    "except:\n",
    "    print (\"unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
