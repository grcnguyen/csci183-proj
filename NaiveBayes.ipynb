{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 183 Data Science\n",
    "### Spam Filtering for Short Messages: Naive Bayes\n",
    "#### Ryan Johnson, Grace Nguyen, and Raya Young\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the test data \n",
    "Separate into two arrays: spam and ham. These arrays will be processed individually in order to generate word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data/spamcollectiondata.tsv\", sep='\\t', names = [\"Category\", \"Message\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Training Data 2\n",
    "The first training data had a small spam sample, but a large ham sample, so we need a sample with lots of spam to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2_messages = list()\n",
    "d2_cats = list()\n",
    "with open(\"training-data/english_big.csv\", \"r\", errors='ignore') as test:\n",
    "    lines = test.readlines()\n",
    "    np.random.shuffle(lines)\n",
    "    for l in lines:\n",
    "        words = l.split(\",\")\n",
    "        message = \"\"\n",
    "        for w in words:\n",
    "            if w == 'ham\\n' or w=='spam\\n':\n",
    "                d2_messages.append(message)\n",
    "                if w == 'ham\\n':\n",
    "                    d2_cats.append('ham')\n",
    "                else:\n",
    "                    d2_cats.append('spam')\n",
    "            else:\n",
    "                message = message + w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase and combine lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "message_data = [word.lower() for word in data['Message']]\n",
    "category = data['Category'].tolist()\n",
    "\n",
    "message_data.extend(d2_messages)\n",
    "category.extend(d2_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal and Stemming\n",
    "Clean both sets of data by removing stopwords. This way, the word cloud will not be completely populated by common stop words. Stemming is also important to ensure eliminate the possibility of having multiple different forms of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' go jurong point, crazy.. avail bugi n great world la e buffet... cine got amor wat...', 'ham'], [' ok lar... joke wif u oni...', 'ham'], [' free entri 2 wkli comp win fa cup final tkts 21st may 2005. text fa 87121 receiv entri question(std txt rate)t&c appli 08452810075over18', 'spam'], [' u dun say earli hor... u c alreadi say...', 'ham'], [\" nah don't think goe usf, live around though\", 'ham'], [\" freemsg hey darl it 3 week word back! i'd like fun still? tb ok! xxx std chgs send, £1.50 rcv\", 'spam'], [' even brother like speak me. treat like aid patent.', 'ham'], [' per request mell mell (oru minnaminungint nurungu vettam) set callertun callers. press *9 copi friend callertun', 'ham'], [' winner!! valu network custom select receivea £900 prize reward! claim call 09061701461. claim code kl341. valid 12 hour only.', 'spam'], [' mobil 11 month more? u r entitl updat latest colour mobil camera free! call mobil updat co free 08002986030', 'spam']]\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "training_set = []\n",
    "i = 0\n",
    "for message in message_data:\n",
    "    sentence = message.split(\" \")\n",
    "    filtered = \"\"\n",
    "    pr = []\n",
    "    for word in sentence:\n",
    "        if word.lower() not in stop:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            filtered = filtered + \" \" + stemmed\n",
    "    pr.append(filtered)\n",
    "    pr.append(category[i])\n",
    "    training_set.append(pr)\n",
    "    i = i+1\n",
    "    \n",
    "#print first 10 elements in training_set to see format\n",
    "print(training_set[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getset():\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the list in half. The first half will be the test set and the second half will be the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = len(training_set)/2\n",
    "test_set = training_set[int(length):]\n",
    "training_set = training_set[:int(length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set(chain(*[word_tokenize(i[0].lower()) for i in training_set]))\n",
    "training_set = [({word: (word in word_tokenize(x[0])) for word in vocab}, x[1]) for x in training_set]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on test half of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for sentence in test_set:\n",
    "    test_sentence = clean(sentence[0])\n",
    "    test_set_feat = {i:(i in word_tokenize(test_sentence.lower())) for i in vocab}\n",
    "    answer = classifier.classify(test_set_feat)\n",
    "    if answer == sentence[1]:\n",
    "        correct = correct + 1\n",
    "    print(\"Answer: \" + answer + \", Actual: \" + sentence[1])\n",
    "    total = total + 1\n",
    "\n",
    "print(\"Accuracy = \", float(correct)/float(total)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
